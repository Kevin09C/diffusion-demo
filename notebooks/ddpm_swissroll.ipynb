{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4f2046",
   "metadata": {},
   "source": [
    "# Denoising diffusion: 2D Swiss roll\n",
    "\n",
    "Generative modeling with DDPMs is demonstrated on the basis of a 2D version of the good old Swiss roll data set. The example merely serves the purpose of quickly familiarizing with the algorithm and its properties. Most of the model architecture or hyperparameter choices are quite arbitrary.\n",
    "\n",
    "You can run `python train_ddpm_swissroll.py` in the main folder in order to train a simple diffusion model. The script relies on the library Lightning for training, logging and checkpointing. When experimenting with with different model setups, or actually almost always, you may want to monitor the training and validation losses. This can be done through `tensorboard --logdir run/swissroll/` for example.\n",
    "\n",
    "After the model has been trained, it can be imported and tested in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e025d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f7bce5-761e-451e-9c8f-91455728616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from lightning.pytorch import seed_everything\n",
    "\n",
    "from diffusion import make_swissroll_2d, DDPMTab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf44a72-cf74-4790-991f-c92199c1f149",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = seed_everything(111111) # set random seeds manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e6e76",
   "metadata": {},
   "source": [
    "## Swiss roll data\n",
    "\n",
    "A synthetic test set is generated in the following, totalling $N=1000$ samples $\\{\\boldsymbol{x}_{0,i}\\}_{i=1}^N$. They are created with the function `make_swissroll_2d`, which is called with the same parameter values that were already used for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983ba85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = make_swissroll_2d(\n",
    "    num_samples=1000,\n",
    "    noise_level=0.5,\n",
    "    scaling=0.15,\n",
    "    val_size=None,\n",
    "    as_tensor=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e923e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ax.scatter(\n",
    "    x_test[:,0].numpy(), x_test[:,1].numpy(), s=20,\n",
    "    edgecolors='none', alpha=0.7, color=plt.cm.cividis(0.2)\n",
    ")\n",
    "ax.set(xlim=(-2.3, 2.7), ylim=(-2.4, 2.6))\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.grid(visible=True, which='both', color='gray', alpha=0.2, linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39de1793",
   "metadata": {},
   "source": [
    "## DDPM import\n",
    "\n",
    "Our trained model is now imported. A `DDPMTab`-object is initialized accordingly to that end. It provides methods for the forward and reverse process and allows for computing the loss $L_\\text{simple} = \\mathbb{E}_{\\mathcal{U}(t|1, T), q(\\boldsymbol{x}_0), \\mathcal{N}(\\boldsymbol{\\epsilon} | \\boldsymbol{0}, \\boldsymbol{I})}[\\lVert \\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\boldsymbol{\\theta}(\\sqrt{\\bar{\\alpha}_t} \\boldsymbol{x}_0 + \\sqrt{1-\\bar{\\alpha}_t} \\boldsymbol{\\epsilon}, t) \\rVert^2]$. The diffusion process consists of $T=500$ time steps. Note that, very probably, one could even get away with a smaller number. The noise model $\\boldsymbol{\\epsilon}_\\boldsymbol{\\theta}(\\boldsymbol{x}_t, t)$ is composed of fully connected layers and ReLU activations. Every layer is explicitly conditioned on the time $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2484b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_file = '../run/swissroll/version_0/checkpoints/last.ckpt'\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "checkpoint = torch.load(ckpt_file, map_location=device)\n",
    "ddpm = DDPMTab.load_from_checkpoint(ckpt_file)\n",
    "\n",
    "ddpm = ddpm.eval()\n",
    "ddpm = ddpm.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0426c45",
   "metadata": {},
   "source": [
    "The noise schedule represents an important model setting. Therefore, $\\beta_t$ and $\\sqrt{\\bar{\\alpha}_t}$ as well as the log-SNR corresponding to the selected schedule are plotted below for all $t=1,\\ldots,500$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4fa0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = ddpm.betas.cpu().numpy()\n",
    "alphas_bar = ddpm.alphas_bar.cpu().numpy()\n",
    "log_snr = np.log(alphas_bar / (1 - alphas_bar))\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(9, 2.5))\n",
    "\n",
    "ax1.plot(np.arange(len(betas)) + 1, betas, color=plt.cm.Set1(1))\n",
    "ax1.set(xlim=(0, len(ddpm.betas)), ylim=(0, ddpm.betas.max().item()))\n",
    "ax1.set(xlabel='t', ylabel='$\\\\beta$')\n",
    "ax1.grid(visible=True, which='both', color='gray', alpha=0.2, linestyle='-')\n",
    "ax1.set_axisbelow(True)\n",
    "\n",
    "ax2.plot(np.arange(len(alphas_bar)) + 1, np.sqrt(alphas_bar), color=plt.cm.Set1(2))\n",
    "ax2.set(xlim=(0, len(ddpm.alphas_bar)), ylim=(0, 1))\n",
    "ax2.set(xlabel='t', ylabel='$\\\\sqrt{\\\\bar{\\\\alpha}}$')\n",
    "ax2.grid(visible=True, which='both', color='gray', alpha=0.2, linestyle='-')\n",
    "ax2.set_axisbelow(True)\n",
    "\n",
    "ax3.plot(np.arange(len(log_snr)) + 1, log_snr, color=plt.cm.Set1(3))\n",
    "ax3.set(xlim=(0, len(ddpm.alphas_bar)))\n",
    "ax3.set(xlabel='t', ylabel='log-SNR')\n",
    "ax3.grid(visible=True, which='both', color='gray', alpha=0.2, linestyle='-')\n",
    "ax3.set_axisbelow(True)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787f2a03",
   "metadata": {},
   "source": [
    "## Forward process\n",
    "\n",
    "It is interesting to visualize the forward diffusion process applied to the problem at hand. To that end, the training data are diffused step by step according to $q(\\boldsymbol{x}_t | \\boldsymbol{x}_{t-1}) = \\mathcal{N}(\\boldsymbol{x}_t | \\sqrt{1-\\beta_t} \\boldsymbol{x}_{t-1}, \\beta_t \\boldsymbol{I})$ with the method `diffuse_all_steps`. Intermediate results for certain time steps can then be plotted. This allows us to observe the advancing diffusion process from data to pure noise \"in action\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d01456",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_noisy = ddpm.diffuse_all_steps(x_test.to(device)).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8798f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_steps = [0, 10, 20, 50, 100, 150, 500]\n",
    "colors = plt.cm.cividis(np.linspace(0.2, 0.8, len(plot_steps)))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(plot_steps), figsize=(9, 2))\n",
    "for time_idx, color, ax in zip(plot_steps, colors, axes.ravel()):\n",
    "    samples = x_noisy[time_idx].numpy()\n",
    "    ax.scatter(samples[:,0], samples[:,1], s=10, edgecolors='none', alpha=0.5, color=color)\n",
    "    ax.set(xlim=(-2.3, 2.7), ylim=(-2.4, 2.6))\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    ax.set_title('{} steps'.format(time_idx))\n",
    "    ax.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd51974c",
   "metadata": {},
   "source": [
    "It is noted that the simulation of $q(\\boldsymbol{x}_t | \\boldsymbol{x}_0) = \\mathcal{N}(\\boldsymbol{x}_t | \\sqrt{\\bar{\\alpha}_t} \\boldsymbol{x}_0, (1-\\bar{\\alpha}_t) \\boldsymbol{I})$ at any time step directly is enabled by the method `diffuse`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da5fa56",
   "metadata": {},
   "source": [
    "## Generative process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e484490f",
   "metadata": {},
   "source": [
    "After having trained the model, the denoising process can be used for data generation. The `denoise_all_steps` method allows one to perform $p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{t-1} | \\boldsymbol{x}_t) = \\mathcal{N}(\\boldsymbol{x}_{t-1} | \\boldsymbol{\\mu}_\\boldsymbol{\\theta}(\\boldsymbol{x}_t, t), \\sigma_t^2 \\boldsymbol{I})$ with $\\boldsymbol{\\mu}_\\boldsymbol{\\theta}(\\boldsymbol{x}_t, t) = \\frac{1}{\\sqrt{\\alpha_t}}(\\boldsymbol{x}_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_\\boldsymbol{\\theta}(\\boldsymbol{x}_t, t))$ in step-wise fashion. This way, the progressive generation can be visualized for intermediate steps. The evolution from pure noise into the targeted spiral-shaped structure can be observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e59fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_noise = torch.randn(1000, 2)\n",
    "x_denoise = ddpm.denoise_all_steps(x_noise.to(device)).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fba144",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_plot_steps = [ddpm.num_steps - s for s in reversed(plot_steps)]\n",
    "reverse_colors = plt.cm.magma(np.linspace(0.8, 0.2, len(reverse_plot_steps)))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(reverse_plot_steps), figsize=(9, 2))\n",
    "for time_idx, color, ax in zip(reverse_plot_steps, reverse_colors, axes.ravel()):\n",
    "    samples = x_denoise[time_idx].numpy()\n",
    "    ax.scatter(samples[:,0], samples[:,1], s=10, edgecolors='none', alpha=0.5, color=color)\n",
    "    ax.set(xlim=(-2.3, 2.7), ylim=(-2.4, 2.6))\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    ax.set_title('{} steps'.format(time_idx))\n",
    "    ax.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d359e",
   "metadata": {},
   "source": [
    "The same process is also implemented by `generate`, without storing intermediate results though. We use this method to generate some final samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222bd43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gen = ddpm.generate(sample_shape=(2,), num_samples=1000).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41722b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ax.scatter(\n",
    "    x_gen[:,0].numpy(), x_gen[:,1].numpy(), s=20,\n",
    "    edgecolors='none', alpha=0.7, color=reverse_colors[-1]\n",
    ")\n",
    "ax.set(xlim=(-2.3, 2.7), ylim=(-2.4, 2.6))\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.grid(visible=True, which='both', color='gray', alpha=0.2, linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e578aa",
   "metadata": {},
   "source": [
    "Well, it seems to work!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
